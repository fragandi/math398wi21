\documentclass{article}
\usepackage[utf8]{inputenc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Redefine margins and other page formatting



%packages I need



\usepackage{tikz}
\usepackage{tikz-cd}


\usetikzlibrary{arrows,decorations.pathmorphing,decorations.pathreplacing,positioning,shapes.geometric,shapes.misc,decorations.markings,decorations.fractals,calc,patterns}

\usepackage{graphics}
\usepackage[margin=1in]{geometry}
\usepackage[all]{xy}
\usepackage{amsrefs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsxtra}     % Use various AMS packages


\usepackage{epsfig}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Various theorem environments. All of the following have the same numbering
% system as theorem.

\theoremstyle{plain}
\newtheorem{Theorem}{Theorem}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Question}[Theorem]{Question}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\newtheorem{Assumption}[Theorem]{Assumption}
\newtheorem{Algorithm}[Theorem]{Algorithm}

\theoremstyle{definition}
\newtheorem{Definition}[Theorem]{Definition}
\newtheorem{Notation}[Theorem]{Notation}
\newtheorem{Condition}[Theorem]{Condition}
\newtheorem{Example}[Theorem]{Example}
\newtheorem{Introduction}[Theorem]{Introduction}

\theoremstyle{remark}
\newtheorem{Remark}[Theorem]{Remark}
\include{header}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \numberwithin{Theorem}{chapter}        % Numbers theorems "x.y" where x
                                        % is the section number, y is the
                                        % theorem number

% \renewcommand{\thechapter}{\arabic{chapter}}
  
% \renewcommand{\thetheorem}{\roman{chapter}.\arabic{theorem}}

%\makeatletter                          % This sequence of commands will
%\let\c@equation\c@theorem              % incorporate equation numbering
%\makeatother                           % into the theorem numbering scheme

%\renewcommand{\theenumi}{(\roman{enumi})}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% When you first define a new word, use this macro to make it stand out
% EG We say that an abelian group $I$ is \newword{injective} if, for any
% injection $G \to H$, and any map $G \to I$, there is a map $H \to I$ making the
% obvious diagram commute.

\newcommand{\newword}[1]{\textbf{\emph{#1}}}

%Arrows
\newcommand{\into}{\hookrightarrow}
\newcommand{\onto}{\twoheadrightarrow}

%Things LaTeX names by appearance, rather than meaning
% By now, I've learned the standard LaTeX names, but I remember they used to give me trouble, so here are some macros
%You can create your own macros with newcommand

\newcommand{\isom}{\cong} %The isomorphism symbol
\newcommand{\union}{\cup}
\newcommand{\intersection}{\cap}
\newcommand{\bigunion}{\bigcup}
\newcommand{\bigintersection}{\bigcap}
\newcommand{\disjointunion}{\sqcup}
\newcommand{\bigdisjointunion}{\bigsqcup}


%with DeclareMathOperator you can define a new command which in math environment looks like a mathematical function

%Some multiletter functions
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\CoKer}{CoKer}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\MaxSpec}{MaxSpec}
\DeclareMathOperator{\Proj}{Proj}
\DeclareMathOperator{\Frac}{Frac}
\renewcommand{\Im}{\mathop{\mathrm{Im}}}


\DeclareMathOperator{\Obj}{Obj}
\DeclareMathOperator{\kernel}{ker}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\spa}{span}
\DeclareMathOperator{\init}{in}
\DeclareMathOperator{\rk}{rk}


%Their calligraphic versions; use these for the sheaf constructions
\DeclareMathOperator{\HHom}{\mathcal{H}\emph{om}}
\DeclareMathOperator{\EExt}{\mathcal{E}\emph{xt}}
\DeclareMathOperator{\EEnd}{\mathcal{E}\emph{nd}}
\DeclareMathOperator{\TTor}{\mathcal{T}\emph{or}}
\DeclareMathOperator{\KKer}{\mathcal{K}\emph{er}}
\DeclareMathOperator{\CCoKer}{\mathcal{C}\emph{o}\mathcal{K}\emph{er}}
\newcommand{\IIm}{\mathop{\mathcal{I}\emph{m}}}
\DeclareMathOperator{\codim}{codim}



%This makes alternating tensors look right in displayed equations
\newcommand{\Alt}{\bigwedge\nolimits}


%Blackboard bold letters.
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\MM}{\mathbb{M}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\UU}{\mathbb{U}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}

%Calligraphic letters

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

% I used a macro for Id. If one writes $Id$ the "I" and "d" are far apart, because Latex interprets it a product of I and d. 
\DeclareMathOperator{\Id}{Id}

% some other random commands 

\newcommand{\plu}{Pl\"ucker }
\newcommand{\gro}{Gr\"obner }

\newcommand{\N}{\mathbb{N}^n_0}
\newcommand{\G}{G= \{g_1, \ldots, g_t \}}
\newcommand{\p}{\partial}

\newcommand{\sub}{\subset}
\newcommand{\cont}{\supset}


\newcommand{\R}{\KK[x_1, \ldots, x_n]}
\newcommand{\E}{\we(x_1, \ldots, x_n)}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xa}{\x^{\mathbf{a}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ya}{\y^{\mathbf{a}}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfc}{\mathbf{c}}
\newcommand{\bfd}{\mathbf{d}}
\newcommand{\bfz}{\mathbf{0}}

%I can never spell lambda
\newcommand{\la}{\lambda}


\title{Automatic Geometric Theorem Proving}
\author{Sam Ratliff}
\date{19 March 2021}
\begin{document}

\maketitle



\section{Automatic Geometric Theorem Proving with Gr√∂bner Bases}

\subsection{Notation and Intuition}
In any geometric construction, some decisions are arbitrary, like where we place our origin or how long we draw our first line.
However, later in the construction, other decisions will depend on choices we made earlier. 
After all, if you're drawing a triangle and you've already drawn two sides of it, you're rather limited in the options that you have to draw the last side.
In order to reflect this distinction in our proofs,
we will use the variables $u_1,\dots,u_m$ to indicate a variable that has no explicit restriction on its value and 
we will use the variables $x_1,\dots,x_n$ to indicate a dependant variable that has a value that is defined by the values of the variables $u_1,\dots, u_m$. 

As an example, consider four equally spaced points on the $x$-axis of the Cartesian plane. Then, two of these points trisect the line segment created by the first two. 
Call these points $A$, $B$, $M_1$ and $M_2$. 
For the sake of convenience, take $A = (0,0)$.  Then, take $B = (u_1, 0)$. Now, the locations of $M_1$ and $M_2$ have been determined by the selection of $B$.
Thus, we denote $M_1$ as $(x_1,0)$ and $M_2$ as $(x_2,0)$.

Based on our construction, we know these points are equally spaced. Thus, $AM_1 = M_1M_2 = M_2B$.
We can express these relations by writing $x_1 - 0 = x_2  - x_1 = u_1 - x_2$, which, after rearranging, yields the equations below.
\begin{align*}
    h_1 & = x_2 - 2x_1 = 0\\
    h_2 & = u_1 - 2x_2 + x_1 = 0\\
    h_3 & = u_1 - x_2 - x_1 = 0
\end{align*}

The relations above do not explicitly relate the length of one of the smaller segments like $\overline{AM_1}$ to the length of $\overline{AB}$. 
However, we have enough information to show that $3\cdot AM_1 = AB$.

When we encode $3\cdot AM_1 = AB$ as a polynomial, we get the equation $3 (x_1 - 0) = u_1 - 0$. By rearranging, we get the polynomial $g = u_1 - 3x_1 = 0$.
Notice that
\begin{align*}
    h_1 + h_3 & = (u_1 - x_2 - x_1) + (x_2 - x_1)\\
              & = u_1 - 3x_1\\
              & = g
\end{align*}
Therefore, $g$ is a linear combination of our hypotheses and the equation $g = 0$ holds for all solutions to the system of equations $h_1 = h_2 = h_3 = 0$.
Thus, $g \in \langle h_1,h_2,h_3 \rangle$.

We can easily check whether a polynomial is a member of an ideal by dividing it by the \gro basis of the ideal.
This is a much more reliable method than eyeballing our equations and figuring out whether $g$ can be written as a linear combination of our hypotheses.

\subsection{Translating Theorems into Polynomials}
As shown in the example above, we can translate many geometric statements into polynomials. 
This is because many geometric statements are asserting some form of equivalence between two quantities.
In the example above, we equated the distances between points in order to represent two line segments having the same length.
Similarly, we can equate slopes of lines to indicate that they are parallel and we can use mathematical objects like dot products and cross products to measure angles between line segments.
More complicated geometric statements, like those relating to collinearity, midpoints, circles, and other common geometric constructs, 
can be synthesized by modifying and combining the previous definitions.
For example, three collinear points can be expressed by the relation $AB = AM + MB$, which is a modification of the critereon for two lines having the same length.

\subsection{Fun with Parallelograms and Irreducible Varieties}
In \texttt{6.4.11.m2}, I follow the basic structure of the proof of Example 1 through page 326. I encourange the reader to follow along in the code as they read this section.
The proof begins by establishing our hypotheses and conclusions.
However, the book and my code both quickly find that $g$ does not strictly follow from our hypotheses.
The reason that this occurs is because the variety associated with the ideal of hypotheses is reducible.
We can readily see this by factoring the generators of our \gro basis. 

In order to reduce our \gro basis, we factor all of the generators. 
Then, we two new \gro bases, where each new \gro basis uses all of the same generators as the previous one, except it replaces a factorable generator by one of its factors.
The first generator that I factored was $f_2$, which can be reexpressed as $u_3(x_1 - u_1 - u_2)$.
To reduce this basis, we compute new \gro bases of the ideals $\langle f_1, x_1 - u_1 - u2, f_3, f_4, f_5, f_6 \rangle$ and $\langle f_1, u_3, f_3, f_4, f_5, f_6 \rangle$.
This yields more generators, which we can factor again. We repeat this process until our generators are not factorable. 
We can select a set of \gro bases from our final collection such that the varieties that they are associated with are the irreducible components of $V$.
This will make our subsequent calculations easier while simultaneously explaining why $g$ did not follow strictly from our hypotheses.

The varieties $U_1$, $U_2$, and $U_3$ all have some polynomial in their generators that depends only on a $u$ variable.
Consider $U_1$. The \gro basis of $U_1$ is $\{ x_2, x_4, u_3\}$, which corresponds to the set of equations $x_2 = x_4 = u_3 = 0$. 
By referring back to the definitions of these variables, we can see that solutions that are in $U_1$ correspond to the ``parallelogram" in which the point $C$ is on the $x$-axis.
Likewise, this forces the points $D$ and $N$ to be on the $x$-axis as well.
There's nothing mathematically wrong with a solution like this. 
However, the solutions that it finds do not lead to a parallelogram, which makes it rather difficult to say anything meaningful about the diagonals of said parallelogram.

If we recall our distinction between the $u$ variables and the $x$ variables, this conclusion makes sense.
When setting up our geometric system, we specifically chose values our $u$ variables that produce a system we want. 
Then, the values of our $x$ variables were determined completely by our selection of $u$ variables.
Therefore, if we are not careful about our selection of $u$ variables, we may end up with solutions that fail to have meaning about our system.
After all, the computer sees all of the variables as essentially identical, regardless of whether they were named with a $u$ or an $x$. 
It's up to the programmer to ensure that the results have the expected meaning.

\subsection{Degenerate Solutions}

We call solutions like those found in $U_1$ \textit{degenerate}. If we want to find solutions that correspond to the system that we are interested in, we have to exclude degenerate solutions.
In the case of $U_1$, we noticed that $u_3$ was one of our generators. 
If any of our generators are composed wholly of $u$ variables, then some of the solutions in that variety are related to an incorrectly set up system.
In our case, this improper system will likely be a line.
In other systems, a degenerate solution will reflect some other loss of information encoded in the hypotheses.

To avoid degenerate solutions, we look for irreducible varieties that have no generators that depend only on $u$ variables. 
We say that $u_1,\dots u_m$ are algebraically independant on an irreducible variety if this is true.
In our example, we can see that the only irreducible variety of $V$ that the $u$ variables are algebraically independant over is the variety $V'$.
Thus, we will not have to worry about degenerate solutions appearing on $V'$. 
As such, we can use the generators of the \gro basis of $V'$ to determine whether $g$ follows from the hypotheses.


\subsection{Better Hypotheses}
The hypotheses in this example come in pairs. The first two hypotheses describe the location of the point $D$, while the second pair describe the point $N$.
The book presents a second set of hypotheses that replace the hypotheses that describe the location of $D$.
Instead of using slopes to locate $D$, the book instead looks at the diagram and notes that $(u_1 + u_2,u_3) = (x_1,x_2)$. 
This yields the two hypotheses $h_1'= x_1 - u_1 - u_2 = 0$ and $h_2'= x_2-u_3 = 0$. These two hypotheses are much simpler than the hypotheses that we used initially.
In particular, $h_2'$ does not have a multiplication in it. 
This simplifies the process of reducing the variety into irreducible components because our reductions required us to factor the generators.
By choosing better hypotheses, we avoid having to factor and reduce a variety at least once. 
In fact, I only had to reduce the variety generated by these new hypotheses twice in order to find $V'$, whereas I had to reduce the variety five times with the orignal set of hypotheses.
Thus, picking a good hypothesis set can potentially halve the amount of work that has to be done.


\section{Automatic Geometric Theorem Proving with Wu's Method}
Wu's method is a method of automatic geometric theorem proving that predates the \gro basis method discussed above. 
It uses a method similar to row reduction in matricies to produce a modified version of the hypothesis set that has useful properties that allow us to quickly test a conclusion.
Much like row reduction, we have a rule for how we can combine two entries in our set, which we call pseudodivision.
\subsection{Pseudodivision}
Pseudodivision behaves similar to one variable polynomial division, even though it happens on multivariate rings. Let $f$ and $g$ be polynomials in the field $k[x_1,\dots,x_n,y]$.
First, we select a variable $y$ to pseudodivide with respect to. Then, we rewrite $f$ and $g$ in the form 
\begin {align*}
    f & = c_py^p + \cdots + c_1y + c_0\\
    g & = d_my^m + \cdots + d_1y + d_0
\end{align*}
This allows us to isolate $y$ in each monomial. It is important to note that in normal one variable polynomial division, the coefficients of $y$ are in the field $k$. 
In polynomial pseudodivision, however, the coefficients of $y$ are polynomials in $k[x_1,\dots,x_n]$.

We also need to add an additional condition to single variable polynomial division for pseudodivision to be useful. 
To ensure that the pseudodivision works, we multiply $f$ by a power of $d_m$. 
We do this to ensure that the leading term of $d_m f$, the polynomial that we are dividing, is divisible by the leading term of $g$, the polynomial that we are dividing by. 
In other words, we do not know if $c_p$ is divisible by $d_m$, but we do know that there exists some $s \geq 0$ such that $d_m^s\cdot c_p$ is divisible by $d_m$.
While $s = 1$ will easily satisfy this relationship for the first leading terms of $f$ and $g$, we may need to pick a larger $s$ for terms later in the polynomial.

We can also think about the multiple of $d_m^s$ in another way. 
Instead of multiplying by a power of $d_m$ before dividing, we use one variable polynomial division with respect ot $y$ to divide $f$ by $g$ and we don't worry about whether $c_p$ is divisible by $d_m$. 
This means that some of the coefficients of our pseudoquotient and pseudoremainder may end up being rational polynomials. 
To rectify this, we multiply both sides by the smallest power of $d_m$ that will cancel the denominators that still remain in the pseudoquotient and pseudoremainder.
\begin{Proposition}[Pseudodivision]
Let $f, g \in k[x_1,\dots,x_n,y]$, where $deg(g,y) \leq deg(f,y)$. Then, in order to isolate $y$, rewrite $f$ and $g$ as follows:
\begin {align*}
    f & = c_py^p + \cdots + c_1y + c_0\\
    g & = d_my^m + \cdots + d_1y + d_0
\end{align*}
Then, there exists an equation of the form $$d_m^s f = qg + r,$$ where $q,r \in k[x_1,\dots,x_n,y]$, $s \geq 0$, and either $r = 0$ or $deg(r,y) < m$.
\end{Proposition}

We denote the pseudoremainder $r$ of $f$ pseudodivided by $g$ with respect to $y$ as $Rem(f,g,y)$.
One nice property of the pseudoremainder is that, as stated in the propositoin above, $deg(r,y) < deg(f,y) = m$. 
Thus, if we replace $f$ with $r$ and repeatedly pseudodivide, we can eliminate the variable $y$ from an equation without losing other important information about it. 
This operation is similar to mapping a number to its equivalence class modulo $n$. 
In the case of integer equivalence classes, we are only interested in the remainder, which functionally behaves the same as the original integer under the operations of the ring.

\subsection{Triangularization}
Let $f_1',\dots,f_n' \in k[u_1,\dots,u_m,x_1,\dots,x_n]$ be the hypotheses derived from our geometric system.
By using pseudodivision, we can reduce these hypotheses down to triangular form. To do this, we take the $f_i'$ that has the lowest nonzero degree in $x_n$ and swap it with $f_n'$.
Then, for each $f_j'$ where $j < n$, if $f_j'$ depends on $x_n$, we replace $f_j'$ with $Rem(f_j', f_n, x_n)$. We repeat this process until $f_n'$ is the only hypothesis that depends on $x_n$. 
Then, we find the $f_i'$ that has the lowest nonzero degree in $x_{n-1}$ and swap it with $f_{n-1}'$.
We repeat this process until we have reached a set of hypotheses of the form
\begin{align*}
    f_1' & = f_1'(u_1,\dots,u_m,x_1)\\
    f_2' & = f_2'(u_1,\dots,u_m,x_2)\\
    \vdots & \\
    f_n' & = f_n'(u_n,\dots,u_m,x_1,\dots,x_n)
\end{align*}

\subsection{Successive Pseudodivision}

A quick note about notation: unfortunately, the book uses $f$ and $g$ to introduce pseudodivision. We divide $f$ by $g$ in the definition of pseudodivision.
It also uses the set of $f_1,\dots,f_n$ to represent the triangularized polynomials and uses $g$ to represent the conclusion of our proof.
This means that we end up dividing $g$ by the set of $f_1,\dots,f_n$ in this section, which is the opposite of what we had before. 
This tripped me up on my first reading, so I wanted to clarify that point here. Anyway, back to the math.

Much like in the triangularization procedure, we can use pseudodivision on any polynomial in order to eliminate the variables $x_1,\dots,x_n$ from its remainder.
We do this with the following process.

Let $R_n = g$. Then, compute $R_{i-1} = Rem(R_i,f_i,x_i)$. Each successive pseudodivision will eliminate each $x_i$. 
Additionally, we know that each $x_i$ will not be reintroduced in a later pseudodivision because the system has been triangularized.
Thus, when we reach $R_0$, all of the $x_i$s will have been eliminated and we will have a polynomial that depends only on $u_1,\dots,u_m$.


\subsection{Wu's Method}
Wu's method lets us use pseudodivision to determine whether a conclusion $g$ follows from a triangularized set of hypotheses $f_1,\dots,f_n$.
Once again, however, Wu's method is not guaranteed to work on reducible varieties. As such, decomposing a variety into its irreducible components and determining $V'$ may still be required.
\begin{Theorem}[Wu's Method]
    Let $R_0$ be the final remainder computed by the successive pseudodivision of $g$ by the set of triangularized hypotheses $f_1,\dots,f_n$ as discussed above.
    Let $d_i$ be the leading coefficient of each $f_i$. 
    Then, there are nonnegative integers $s_1,\dots,s_n$ and polynomials $A_1,\dots,A_n$ in the ring $\mathbb{R}[u_1,\dots,u_m,x_1\dots,x_n]$ such that
    $$d_{1}^{s_1}\cdot d_2^{s_2} \cdots d_n^{s_n} g = A_1 f_1 + \cdots + A_n f_n + R_0.$$
    Additionally, if $R_0$ is the zero polynomial, then $g$ is zero at every point of $V' \setminus \textbf{V}(d_1d_2\cdots d_n) \subseteq \mathbb{R}^{m+n}$.
\end{Theorem}
The proof of the first part of this theorem is rather intuitive. 
By rearranging the definition of pseudodivision, we can see that after our first step of successive pseudodivision, we can see that $$R_{n-1} = d_n^{s_n} g - q_nf_n.$$
When we pseudodivide again, we get
\begin{align*}
    R_{n-2} & = d_{n-1}^{s_{n-1}}(d_n^{s_n} g - q_nf_n) - q_{n-1}f_{n-1}\\
            & = d_{n-1}^{s_{n-1}}d_n^{s_n} g - q_{n-1}f_{n-1} - d_{n-1}^{s_{n-1}}q_nf_n \\
\end{align*}
From here, it is apparent that the $d_i^{s_i}$ terms will accumulate in front of $g$ and each $f_i$ will also be multiplied by some polynomial.

If $R_0$ is the zero polynomial, then $$d_{1}^{s_1} \cdots d_n^{s_n} g = A_1 f_1 + \cdots + A_n f_n.$$
Therefore, for every point on the variety $\textbf{V}(f_1,\dots,f_n)$, each $f_i = 0$, so at least one polynomial on the left side of the equation must equal zero. 

We know that since $V'$ is an irreducible component of $\textbf{V}(f_1,\dots,f_n)$, it must hold that $V' \subseteq \textbf{V}(f_1,\dots,f_n)$.
Once again, for all points on $V'$, each $f_i = 0$, so it also holds on the points in $V'$ that at least one polynomial on the left side of the equation must equal zero.  
Thus, by removing the points where any $d_i = 0$ from our variety, we find that $g = 0$ on $V' \setminus \textbf{V}(d_1d_2\cdots d_n)$. 

I wrote a basic implementation of the pseudodivision algorithm in the file \texttt{6.5.6.m2} and used it to prove Example 1 from Section 6.4. 
I used this implementation to follow Wu's method for triangularizing the hypotheses and testing the conclusions of the parallelogram proof.
I found that once I had set up my pseudodivision algorithm, which was a more arduous process than I had anticipated, 
the process went very smoothly and was much less time consuming to complete than the original method presented by the book in 6.4.
One fortunate occurance was that, even though the variety $V$ was not reduced into its components, Wu's method still worked on it.
Wu's method is proven to work on irreducible varieties on which the $u$ variables are algebraically independant.
As such, Wu's method may work on an unreduced variety, but it is not a necessary condition.

Wu's method and its variants are generally recognized as the computationally preferable automatic geometric theorem prover and are used much more often than the Gr√∂bner basis method.
This is because the triangularization and pseudodivision algorithms are significantly less computationally demanding than determining the Gr√∂bner basis of an ideal.
The \gro basis of an ideal preserves many more nice properties of the ideal than the triangularization procedure. 
It is unsurprising, then, that computing a \gro basis is a more intricate and time-consuming process than triangularizing the same set of functions.
Since the added benefits of using a \gro basis are not used in automatic geometric theorem proving, it is more computationally efficient to use a solver like Wu's method.

\end{document}
